---
title: "MDTS4214_739_KNN regression"
author: "Debkanta Ghosh"
date: "2026-02-26"
output: word_document
---
## Question 1
  
  Consider a setting with  
\[
  n = 1000
  \]
observations generated as follows:
  
  \[
    x_{1i} \sim N(0, 2^2), \quad x_{2i} \sim \text{Poisson}(\lambda = 1.5),
    \]
\[
  \varepsilon_i \sim N(0,1),
  \]
\[
  y_i = -2 + 1.4x_{1i} - 2.6x_{2i} + \varepsilon_i.
  \]

```{r}
rm(list=ls())
set.seed(123)
n=1000
x1=rnorm(n,0,2)
x2=rpois(n,1.5)
e=rnorm(n)
y=-2+1.4*x1-2.6*x2+e
```

The first 800 observations are used as the training set and the remaining 200 observations are used as the test set.

```{r}
train.data=data.frame(y[1:800],x1[1:800],x2[1:800])
colnames(train.data)=c("y","x1","x2")
test.data=data.frame(y[801:1000],x1[801:1000],x2[801:1000])
colnames(test.data)=c("y","x1","x2")
#Train Data Overview
head(train.data)

#Test Data overview
head(test.data)
```


### (a)
Fit a multiple linear regression model of \(y\) on \(x_1\) and \(x_2\).  
Calculate the test Mean Squared Error (MSE).
```{r}
#1 linear regression
fit1=lm(y~x1+x2,data=train.data)
summary(fit1)
```

Fitted model is given by,

$$
\hat{y} = -2.07300 + 1.38207\,x_1 - 2.55584\,x_2
$$
with multiple R-sqaured = 0.9492. So this is a very good fit.

Calculate the test Mean Squared Error (MSE).

```{r}
y.hat.test=predict(fit1,newdata=test.data)
mse.test=mean((test.data$y-y.hat.test)^2)
mse.test
```
Test MSE comes out as 0.998901 which is very low, implicating that the model is good.
### (b)
Fit a K-Nearest Neighbours (KNN) regression model for  
\[
  k = 1, 2, 5, 9, 15.
  \]
Calculate the test MSE for each value of \(k\).

```{r}
k=c(1,2,5,9,15)
library(FNN)
X.train=matrix(c(x1[1:800],x2[1:800]),byrow=F,ncol=2)
X.test=matrix(c(x1[801:1000],x2[801:1000]),byrow=F,ncol=2)
f=function(k)
{
pred=knn.reg(train=X.train,test=X.test,y=train.data$y,k)
mean((test.data$y-pred$pred)^2)
}
test_mse=c(f(1),f(2),f(5),f(9),f(15))
cbind(k,test_mse)
```

For five choices of K, we perform KNN regression on y and observe that for each choice of K, the MSE is low as well. So KNN regression in this case does not provide an improvement over linear regression.

## Question 2

Now suppose the response variable is generated according to the nonlinear model

\[
y_i =
\frac{1}{-2 + 1.4x_{1i} - 2.6x_{2i} + 2.9x_{1i}^2}
+ 3.1\sin(x_{2i})
- 1.5x_{1i}x_{2i}^2
+ \varepsilon_i.
\]


```{r}
y.new=1/(-2+1.4*x1-2.6*x2+2.9*x1*x1)+3.1*sin(x2)-1.5*x1*x2*x2+e
train.data.new=data.frame(y.new[1:800],x1[1:800],x2[1:800])
colnames(train.data.new)=c("y","x1","x2")
test.data.new=data.frame(y.new[801:1000],x1[801:1000],x2[801:1000])
colnames(test.data.new)=c("y","x1","x2")
head(train.data.new) #Train data overview
head(test.data.new) #test data overview
```


### (a)
Fit a multiple linear regression model of \(y\) on \(x_1\) and \(x_2\).  
Calculate the test Mean Squared Error (MSE).
```{r}
fit3=lm(y~x1+x2,data=train.data.new)
summary(fit3)
```

Fitted model:

$$
\hat{y} = -0.2369 - 6.3747\,x_1 + 1.3849\,x_2
$$

with multiple R-squared = 0.3146 which means this fit is not good.

Then we find out the test mse
```{r}
y.hat.test.new=predict(fit3,newdata=test.data.new)
mse.test.new=mean((test.data.new$y-y.hat.test.new)^2)
mse.test.new
```
Test MSE = 205.1776 means the model is not good.

### (b)
Fit K-Nearest Neighbours regression models for  
\[
k = 1, 2, 5, 9, 15.
\]
Calculate the test MSE for each value of \(k\).



```{r}
library(FNN)
X.train.new=matrix(c(x1[1:800],x2[1:800]),byrow=F,ncol=2)
X.test.new=matrix(c(x1[801:1000],x2[801:1000]),byrow=F,ncol=2)
f2=function(k){
  pred.new=knn.reg(train=X.train.new,test=X.test.new,y=y.new[1:800],k)
  mean((test.data.new$y-pred.new$pred)^2)
}
mse.knn.new=c(f2(1),f2(2),f2(5),f2(9),f2(15))
cbind(k,mse.knn.new)
```
Here we observe that test MSE has been reduced drastically due to KNN regression compared to multiple linear regression.

### (c)
Compare the performance of linear regression and KNN regression models and comment on the results.

Parametric models perform well when the assumed functional form is correctly specified. However, when the true relationship between predictors and the response variable is nonlinear, nonparametric methods such as K-nearest neighbours regression often provide superior predictive performance.